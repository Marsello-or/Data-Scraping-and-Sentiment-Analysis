{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# UAS Akal Imitasi Sentiment Analysis\n",
        "---\n",
        "Marsello Ormanda\n",
        "\n",
        "240310220056"
      ],
      "metadata": {
        "id": "HbWLm1zCIdtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library\n",
        "---\n",
        "Menggunakan library BeautifulSoup dan Transformer dari hugingface"
      ],
      "metadata": {
        "id": "uS5efXA85jjq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk6a54852qMd"
      },
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4 pandas transformers textblob wordcloud --quiet\n",
        "print(\"Instalasi library yang dibutuhkan telah selesai.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFcrg4sc33zI"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "from transformers import pipeline\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Konfigurasi\n",
        "---"
      ],
      "metadata": {
        "id": "gz03RF9jZMye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NPM = \"240310220056\"\n",
        "CSV_FILE_NAME = f\"{NPM}_Limabelas.csv\"\n",
        "WORDCLOUD_FILE_NAME = f\"{NPM}_WordCloud.png\""
      ],
      "metadata": {
        "id": "fgnaMh8mZOTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Soal 1: Mengambil Daftar URL Berita dari Halaman Utama\n",
        "---"
      ],
      "metadata": {
        "id": "5jit9tseZUig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mengakses url web sumber data scraping\n",
        "---"
      ],
      "metadata": {
        "id": "iXi39FuB5zxR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_k0GUV5o4D89"
      },
      "outputs": [],
      "source": [
        "print(\">>> BAGIAN 1: Mengambil daftar URL berita...\")\n",
        "base_url = \"https://lite.cnn.com\"\n",
        "urls_to_scrape = []\n",
        "try:\n",
        "    # Menggunakan headers untuk menyamar sebagai browser agar tidak diblokir\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "    response = requests.get(base_url, headers=headers)\n",
        "    response.raise_for_status() # Cek error\n",
        "\n",
        "    #Parsing website lite cnn\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    all_articles = soup.find_all('li')\n",
        "\n",
        "    if len(all_articles) >= 52:\n",
        "        # Mengambil daftar berita sesuai soal\n",
        "        selected_articles = all_articles[:5] + all_articles[47:52] + all_articles[-5:]\n",
        "        for article in selected_articles:\n",
        "            link_tag = article.find('a')\n",
        "            if link_tag and 'href' in link_tag.attrs:\n",
        "                full_url = base_url + link_tag['href']\n",
        "                if full_url not in urls_to_scrape:\n",
        "                    urls_to_scrape.append(full_url)\n",
        "        print(f\"Selesai. Ditemukan {len(urls_to_scrape)} URL berita unik.\")\n",
        "    else:\n",
        "        print(f\"Peringatan: Hanya ditemukan {len(all_articles)} berita, tidak cukup untuk memenuhi syarat.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Gagal mengakses halaman utama CNN Lite. Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Soal 2 : Mengambil Data Detail dari setiap URL Beritanya\n",
        "---"
      ],
      "metadata": {
        "id": "Ny07I_UXhZ5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n>>> BAGIAN 2: Mengambil detail dari setiap halaman berita...\")\n",
        "all_news_data = []\n",
        "if urls_to_scrape:\n",
        "    # membuat loop untuk iterasi setiap url yang ada di list\n",
        "    for i, url in enumerate(urls_to_scrape, 1):\n",
        "        try:\n",
        "            # parsing dari url yang ada\n",
        "            page = requests.get(url, headers=headers)\n",
        "            soup_detail = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "            # Mengambil Judul\n",
        "            judul_tag = soup_detail.find(\"div\", class_=\"headline--lite\")\n",
        "            judul = judul_tag.find(\"h2\", class_=\"headline\").text.strip() if judul_tag else \"N/A\"\n",
        "\n",
        "            # Mengambil Penulis\n",
        "            penulis_tag = soup_detail.find('p', class_=\"byline--lite\")\n",
        "            penulis = penulis_tag.text.replace(\"By\", \"\").strip() if penulis_tag else \"N/A\"\n",
        "\n",
        "            # KOREKSI 2: Logika baru untuk mengambil Tanggal dan Jam\n",
        "            tanggal = \"N/A\"\n",
        "            jam = \"N/A\"\n",
        "            timestamp_tag = soup_detail.find('p', class_=\"timestamp--lite\")\n",
        "            if timestamp_tag:\n",
        "                full_timestamp_text = timestamp_tag.text.strip()\n",
        "\n",
        "                # Mengambil semua teks di antara ':' pertama dan ',' pertama\n",
        "                if \":\" in full_timestamp_text and \",\" in full_timestamp_text:\n",
        "                    start = full_timestamp_text.find(':') + 1\n",
        "                    end = full_timestamp_text.find(',')\n",
        "                    jam = full_timestamp_text[start:end].strip()\n",
        "\n",
        "                # Mengambil Tanggal: bagian setelah koma pertama\n",
        "                if \",\" in full_timestamp_text:\n",
        "                    tanggal_parts = full_timestamp_text.split(',')[1:]\n",
        "                    tanggal = ','.join(tanggal_parts).strip()\n",
        "\n",
        "            # Mengambil sumber berita\n",
        "            sumber_tag = soup_detail.find('p', class_=\"source--lite\")\n",
        "            sumber = sumber_tag.text.strip() if sumber_tag else \"N/A\"\n",
        "\n",
        "            # Mengambil Baris Pertama Berita\n",
        "            baris_pertama_tag = soup_detail.find('p', class_='paragraph--lite')\n",
        "            baris_pertama = baris_pertama_tag.text.strip() if baris_pertama_tag else \"N/A\"\n",
        "\n",
        "            # Mengambil Link Utuh\n",
        "            link_lengkap = \"N/A\"\n",
        "            # Mencari tag <a> yang teksnya mengandung frasa tertentu\n",
        "            link_lengkap_tag = soup_detail.find('a', string=re.compile(r\"See Full Web Article\"))\n",
        "            if link_lengkap_tag and link_lengkap_tag.has_attr('href'):\n",
        "                link_lengkap = link_lengkap_tag['href']\n",
        "\n",
        "            # Menambahkan semua data yang sudah diekstrak dengan benar ke list\n",
        "            all_news_data.append({\n",
        "                \"Judul\": judul,\n",
        "                \"Penulis\": penulis,\n",
        "                \"Tanggal Terbit\": tanggal,\n",
        "                \"Jam Terbit\": jam,\n",
        "                \"Sumber Berita\": sumber,\n",
        "                \"Baris Pertama Berita\": baris_pertama,\n",
        "                \"Link Berita Utuh\": link_lengkap,\n",
        "                \"Link Berita Lite\": url\n",
        "            })\n",
        "            print(f\"  ({i}/{len(urls_to_scrape)}) Berhasil scrape: {judul}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ({i}/{len(urls_to_scrape)}) Gagal scrape {url}. Error: {e}\")\n",
        "else:\n",
        "    print(\"Tidak ada URL untuk di-scrape.\")"
      ],
      "metadata": {
        "id": "Z8KHMsQ5hArg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cek Hasil Scraping Data Detail Berita\n",
        "---"
      ],
      "metadata": {
        "id": "iFzy0LgSBPum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_news_data"
      ],
      "metadata": {
        "id": "vug7i1lvtNlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_news = pd.DataFrame(all_news_data)\n",
        "\n",
        "print(\"\\nDataFrame Data Detail Berita:\")\n",
        "print(df_news)\n",
        "\n",
        "print(f\"\\nDataFrame memiliki {len(df_news)} baris dan {len(df_news.columns)} kolom.\")"
      ],
      "metadata": {
        "id": "oIpizxO99Nl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_news.isnull()"
      ],
      "metadata": {
        "id": "jkQ3B103Ba57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Terdeteksi tidak ada data yang null atau kosong, sehingga semua detail berita berhasil ter-scrape atau terambil"
      ],
      "metadata": {
        "id": "oTmh4gUmBrtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_news.to_csv('240310220056_Limabelas(without sentiment).csv', index=False)\n",
        "print(\"DataFrame df_news telah disimpan ke dalam file CSV dengan nama 240310220056_Limabelas(wihout sentiment).csv\")"
      ],
      "metadata": {
        "id": "rS6e_-bGGEnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Soal 3 : Analisis Sentimen dengan Text Blob dan juga Model Huggingface\n",
        "---"
      ],
      "metadata": {
        "id": "vosP2h3JCcAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analisis Sentiment text_blob\n",
        "---"
      ],
      "metadata": {
        "id": "Fk30GQknDGML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "polarities = []\n",
        "subjectivities = []"
      ],
      "metadata": {
        "id": "nHvWwxvaB_lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for teks in df_news[\"Baris Pertama Berita\"]:\n",
        "  result = TextBlob(teks)\n",
        "\n",
        "  polarity = result.sentiment.polarity\n",
        "  subjectivity = result.sentiment.subjectivity\n",
        "\n",
        "  polarities.append(polarity)\n",
        "  subjectivities.append(subjectivity)"
      ],
      "metadata": {
        "id": "G3EI2I1eCnIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_news[\"polarity_textblob\"] = polarities\n",
        "df_news[\"subjectivity_textblob\"] = subjectivities"
      ],
      "metadata": {
        "id": "V4p5QBOrCw5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_news.head()"
      ],
      "metadata": {
        "id": "YUbLjRUwC3eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analisis Sentimen dengan Huggingface\n",
        "---"
      ],
      "metadata": {
        "id": "JQEbIOiIDKrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"ProsusAI/finbert\"\n",
        "analyze = pipeline(\"text-classification\", model=model)"
      ],
      "metadata": {
        "id": "aUkMidPYC5a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze(df_news[\"Baris Pertama Berita\"][0])"
      ],
      "metadata": {
        "id": "1UphDNLXDwlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "scores = []\n",
        "\n",
        "for teks in df_news[\"Baris Pertama Berita\"]:\n",
        "  result = analyze(teks)\n",
        "  HF_label = result[0][\"label\"]\n",
        "  HF_score = result[0][\"score\"]\n",
        "\n",
        "  labels.append(HF_label)\n",
        "  scores.append(HF_score)"
      ],
      "metadata": {
        "id": "u4H_OWAxD3fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_news[\"HFLabel\"] = labels\n",
        "df_news[\"HFScores\"] = scores"
      ],
      "metadata": {
        "id": "TS5YAz-4EBc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_news.head()"
      ],
      "metadata": {
        "id": "aqV2vFAaEGJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_news.to_csv('240310220056_Limabelas(with sentiment).csv', index=False)\n",
        "print(\"DataFrame df_news telah disimpan ke dalam file CSV dengan nama 240310220056_Limabelas(with sentiment).csv\")"
      ],
      "metadata": {
        "id": "caP_kE_iGpBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Soal 4 :  WordCloud dari Baris Pertama Berita\n",
        "---"
      ],
      "metadata": {
        "id": "iw_XqmCuETZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gabung semua teks dari kolom Baris Pertama Berita\n",
        "all_text = \" \".join(df_news['Baris Pertama Berita'].dropna())"
      ],
      "metadata": {
        "id": "6_k13whCEesZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat WordCloud dengan ukuran 16:9\n",
        "wordcloud = WordCloud(width=1600, height=900, background_color='white').generate(all_text)"
      ],
      "metadata": {
        "id": "ZPt5J5vtEzCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tampilkan wordcloud\n",
        "plt.figure(figsize=(16, 9))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off') # Jangan tampilkan sumbu\n",
        "plt.title('Word Cloud dari Baris Pertama Berita')"
      ],
      "metadata": {
        "id": "fKVr5StXE1Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan wordcloud ke file\n",
        "plt.savefig(WORDCLOUD_FILE_NAME, dpi=300, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n",
        "print(f\"WordCloud berhasil dibuat dan disimpan sebagai {WORDCLOUD_FILE_NAME}\")"
      ],
      "metadata": {
        "id": "x2JY0JDaEurW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "df_news.to_csv(CSV_FILE_NAME, index=False)\n",
        "\n",
        "print(f\"\\nDataFrame berhasil disimpan ke dalam file CSV: {CSV_FILE_NAME}\")\n",
        "\n",
        "files.download(CSV_FILE_NAME)\n",
        "print(f\"File CSV '{CSV_FILE_NAME}' siap untuk diunduh.\")"
      ],
      "metadata": {
        "id": "0bIMbVmQEHeY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}